{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6962,
     "status": "ok",
     "timestamp": 1743300610801,
     "user": {
      "displayName": "長尾大道",
      "userId": "00877909077498074599"
     },
     "user_tz": -540
    },
    "id": "iDd0Rl3hHh2Y",
    "outputId": "ba867451-a94e-4422-c22a-11f68db915b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seisbench in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.21.6 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (2.2.3)\n",
      "Requirement already satisfied: h5py>=3.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (3.13.0)\n",
      "Requirement already satisfied: obspy>=1.3.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (1.4.1)\n",
      "Requirement already satisfied: tqdm>=4.52 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (2.6.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (1.15.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.3 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (1.6.0)\n",
      "Requirement already satisfied: bottleneck>=1.3 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from seisbench) (1.4.2)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from obspy>=1.3.1->seisbench) (3.10.1)\n",
      "Requirement already satisfied: lxml in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from obspy>=1.3.1->seisbench) (5.3.2)\n",
      "Requirement already satisfied: setuptools in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from obspy>=1.3.1->seisbench) (65.5.0)\n",
      "Requirement already satisfied: sqlalchemy<2 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from obspy>=1.3.1->seisbench) (1.4.54)\n",
      "Requirement already satisfied: decorator in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from obspy>=1.3.1->seisbench) (5.2.1)\n",
      "Requirement already satisfied: requests in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from obspy>=1.3.1->seisbench) (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from pandas>=1.1->seisbench) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from pandas>=1.1->seisbench) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from pandas>=1.1->seisbench) (2025.2)\n",
      "Requirement already satisfied: filelock in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from torch>=1.10.0->seisbench) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from torch>=1.10.0->seisbench) (4.13.1)\n",
      "Requirement already satisfied: networkx in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from torch>=1.10.0->seisbench) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from torch>=1.10.0->seisbench) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from torch>=1.10.0->seisbench) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from torch>=1.10.0->seisbench) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.10.0->seisbench) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from matplotlib>=3.3->obspy>=1.3.1->seisbench) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.1->seisbench) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->seisbench) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from requests->obspy>=1.3.1->seisbench) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from requests->obspy>=1.3.1->seisbench) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from requests->obspy>=1.3.1->seisbench) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tocochan/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages (from requests->obspy>=1.3.1->seisbench) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install seisbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EYrltldYH_Tg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import obspy\n",
    "from obspy import Stream, Trace, UTCDateTime\n",
    "import seisbench.models as sbm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "b0IvjXRIHh2S"
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def convert_ndarry_stream(data, time_str, station_name, sampling_rate=100):\n",
    "    \"\"\"\n",
    "    Convert a 2D NumPy array of waveform data into an ObsPy Stream object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        2D array of shape (3, N), where N is the number of samples for each component (UD, NS, EW).\n",
    "    time_str : str\n",
    "        Start time string in the format 'yymmdd_HHMMSS' (e.g., '150323_141948').\n",
    "    station_name : str\n",
    "        Station name to be assigned to each trace.\n",
    "    sampling_rate : float, optional\n",
    "        Sampling rate in Hz. Default is 100 Hz.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    stream : obspy.Stream\n",
    "        ObsPy Stream object containing three Traces with appropriate metadata.\n",
    "    \"\"\"\n",
    "    year = 2000 + int(time_str[:2])\n",
    "    month, day = int(time_str[2:4]), int(time_str[4:6])\n",
    "    hour, minute, second = int(time_str[7:9]), int(time_str[9:11]), int(time_str[11:13])\n",
    "    utc_time = UTCDateTime(year, month, day, hour, minute, second)\n",
    "\n",
    "    channels = [\"UD\", \"NS\", \"EW\"]\n",
    "    stream = Stream()\n",
    "    for i, ch in enumerate(channels):\n",
    "        trace = Trace(data=data[i, :])\n",
    "        trace.stats.update({\n",
    "            \"sampling_rate\": sampling_rate,\n",
    "            \"starttime\": utc_time,\n",
    "            \"network\": \"MeSO-net\",\n",
    "            \"station\": station_name,\n",
    "            \"location\": \"\",\n",
    "            \"channel\": ch,\n",
    "        })\n",
    "        stream.append(trace)\n",
    "    return stream\n",
    "\n",
    "def convert_stream_to_ndarray(stream, channel_order=[\"UD\", \"NS\", \"EW\"]):\n",
    "    \"\"\"\n",
    "    Convert ObsPy Stream to ndarray of shape (samples, channels).\n",
    "\n",
    "    Parameters:\n",
    "        stream (obspy.Stream): Stream object containing 3 components.\n",
    "        channel_order (list): Order of channels to extract, default is [\"UD\", \"NS\", \"EW\"].\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (3, n_samples)\n",
    "    \"\"\"\n",
    "    traces = []\n",
    "    for ch in channel_order:\n",
    "        tr = stream.select(channel=ch)\n",
    "        if len(tr) == 0:\n",
    "            raise ValueError(f\"Channel {ch} not found in the stream.\")\n",
    "        traces.append(tr[0].data)\n",
    "\n",
    "    # Stack and transpose to shape (samples, channels)\n",
    "    data = np.stack(traces)\n",
    "    return data\n",
    "\n",
    "def calc_snr(signal, noise):\n",
    "    \"\"\"\n",
    "    Calculate signal-to-noise ratio (SNR) in decibels (dB).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    signal : np.ndarray\n",
    "        Array containing the signal portion.\n",
    "    noise : np.ndarray\n",
    "        Array containing the noise portion.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        SNR value in dB.\n",
    "    \"\"\"\n",
    "    return 10 * np.log10(np.std(signal) / np.std(noise))\n",
    "\n",
    "def calc_cc(a, b):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between two signals.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    a : np.ndarray\n",
    "        First signal.\n",
    "    b : np.ndarray\n",
    "        Second signal.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    float\n",
    "        Correlation coefficient between a and b (range: -1 to 1).\n",
    "    \"\"\"\n",
    "    return np.corrcoef(a, b)[0, 1]\n",
    "\n",
    "def zscore(data):\n",
    "    \"\"\"\n",
    "    Normalize each channel using z-score normalization (zero mean, unit variance).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        2D array of shape (channels, time), e.g., (3, N).\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Z-score normalized data with the same shape.\n",
    "    \"\"\"\n",
    "    mean = np.mean(data, axis=1, keepdims=True)\n",
    "    std = np.std(data, axis=1, keepdims=True)\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data\n",
    "\n",
    "def calc_loss(data1, data2, p_onset, s_onset, sf=100):\n",
    "\n",
    "    '''\n",
    "    Calculate a loss value based on signal-to-noise ratio (SNR) and correlation coefficients (CC)\n",
    "    between original and denoised seismic waveform data for P-wave, S-wave, and noise segments.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data1 (Original wave) : np.ndarray\n",
    "        Original waveform data of shape (3, N), where N is the number of time steps.\n",
    "    data2 (Denoised wave) : np.ndarray\n",
    "        Denoised waveform data of shape (3, N), corresponding to data1.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    loss : float\n",
    "        Averaged loss across 3 channels, combining SNR and CC values.\n",
    "    P_SNR : float\n",
    "        Averaged P-wave SNR after denoising.\n",
    "    S_SNR : float\n",
    "        Averaged S-wave SNR after denoising.\n",
    "    P_CC : float\n",
    "        Averaged correlation coefficient between original and denoised P-wave signals.\n",
    "    s_cc : float\n",
    "        Averaged correlation coefficient between original and denoised S-wave signals.\n",
    "    n_cc : float\n",
    "        Averaged correlation coefficient between original and denoised noise segments.\n",
    "    '''\n",
    "\n",
    "    p_snrs = []\n",
    "    s_snrs = []\n",
    "    p_ccs = []\n",
    "    s_ccs = []\n",
    "    n_ccs = []\n",
    "\n",
    "    loss = []\n",
    "\n",
    "    for ch in [0,1,2]:\n",
    "        orig_data = data1[ch,:]\n",
    "        den_data = data2[ch,:]\n",
    "\n",
    "        signal_p = orig_data[p_onset : p_onset+sf*5]\n",
    "        noise_p = orig_data[p_onset-sf*5 : p_onset]\n",
    "\n",
    "        signal_p_deno = den_data[p_onset : p_onset+sf*5]\n",
    "        noise_p_deno = den_data[p_onset-sf*5 : p_onset]\n",
    "\n",
    "        signal_s = orig_data[s_onset : s_onset+sf*5]\n",
    "        signal_s_deno = den_data[s_onset : s_onset+sf*5]\n",
    "\n",
    "        snr_p_orig = calc_snr(signal_p, noise_p)\n",
    "        snr_s_orig = calc_snr(signal_s, noise_p)\n",
    "\n",
    "        snr_p_deno = calc_snr(signal_p_deno, noise_p_deno)\n",
    "        snr_s_deno = calc_snr(signal_s_deno, noise_p_deno)\n",
    "\n",
    "        cc_n = calc_cc(noise_p, noise_p_deno)\n",
    "        cc_p = calc_cc(signal_p, signal_p_deno)\n",
    "        cc_s = calc_cc(signal_s, signal_s_deno)\n",
    "\n",
    "        loss_ch = (snr_p_deno + snr_s_deno) * cc_p * cc_s * cc_n\n",
    "\n",
    "        p_snrs.append(snr_p_deno)\n",
    "        s_snrs.append(snr_s_deno)\n",
    "\n",
    "        p_ccs.append(cc_p)\n",
    "        s_ccs.append(cc_s)\n",
    "        n_ccs.append(cc_n)\n",
    "\n",
    "        loss.append(loss_ch)\n",
    "\n",
    "    return np.mean(loss), p_snrs, s_snrs, p_ccs, s_ccs, n_ccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16135,
     "status": "ok",
     "timestamp": 1743300782349,
     "user": {
      "displayName": "長尾大道",
      "userId": "00877909077498074599"
     },
     "user_tz": -540
    },
    "id": "qcwsDOt6IFR0",
    "outputId": "3632f04b-dca7-45bb-86e8-d6b0a8020592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4936\n",
      "0/4936\n",
      "100/4936\n",
      "200/4936\n",
      "300/4936\n",
      "400/4936\n",
      "500/4936\n",
      "600/4936\n",
      "700/4936\n",
      "800/4936\n",
      "900/4936\n",
      "1000/4936\n",
      "1100/4936\n",
      "1200/4936\n",
      "1300/4936\n",
      "1400/4936\n",
      "1500/4936\n",
      "1600/4936\n",
      "1700/4936\n",
      "1800/4936\n",
      "1900/4936\n",
      "2000/4936\n",
      "2100/4936\n",
      "2200/4936\n",
      "2300/4936\n",
      "2400/4936\n",
      "2500/4936\n",
      "2600/4936\n",
      "2700/4936\n",
      "2800/4936\n",
      "2900/4936\n",
      "3000/4936\n",
      "3100/4936\n",
      "3200/4936\n",
      "3300/4936\n",
      "3400/4936\n",
      "3500/4936\n",
      "3600/4936\n",
      "3700/4936\n",
      "3800/4936\n",
      "3900/4936\n",
      "4000/4936\n",
      "4100/4936\n",
      "4200/4936\n",
      "4300/4936\n",
      "4400/4936\n",
      "4500/4936\n",
      "4600/4936\n",
      "4700/4936\n",
      "4800/4936\n",
      "4900/4936\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Failed to interpret file '../data/Learning/loss_results.csv' as a pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages/numpy/lib/npyio.py:465\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'ileName,LOSS,UD_P_SNR,UD_S_SNR,UD_P_CC,UD_S_CC,UD_N_CC,NS_P_SNR,NS_S_SNR,NS_P_CC,NS_S_CC,NS_N_CC,EW_P_SNR,EW_S_SNR,EW_P_CC,EW_S_CC,EW_N_CC\r\n'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m c % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     23\u001b[39m   \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/Learning\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m+\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m wave, p_onset, s_onset = data[\u001b[33m'\u001b[39m\u001b[33mwave\u001b[39m\u001b[33m'\u001b[39m], data[\u001b[33m'\u001b[39m\u001b[33mpidx\u001b[39m\u001b[33m'\u001b[39m], data[\u001b[33m'\u001b[39m\u001b[33msidx\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     27\u001b[39m time_str, station_name = os.path.basename(fn).replace(\u001b[33m'\u001b[39m\u001b[33m.npz\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m).split(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/self_study/2025_geosci/earthquake/.venv/lib/python3.11/site-packages/numpy/lib/npyio.py:467\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pickle.load(fid, **pickle_kwargs)\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(\n\u001b[32m    468\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to interpret file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m as a pickle\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mUnpicklingError\u001b[39m: Failed to interpret file '../data/Learning/loss_results.csv' as a pickle"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "\n",
    "# Load pretrained denoising model\n",
    "model = sbm.DeepDenoiser.from_pretrained(\"original\")\n",
    "\n",
    "# Prepare data\n",
    "raw_dir = '../data/Learning'\n",
    "files = sorted(os.listdir(raw_dir))     # 適切なパスを設定\n",
    "\n",
    "print(len(files))\n",
    "\n",
    "with open('../data/Learning/loss_results.csv', 'w', newline='') as csvfile:   # 適切なパスを設定\n",
    "    writer = csv.writer(csvfile)\n",
    "\n",
    "    # headder\n",
    "    writer.writerow(['FileName', 'LOSS', 'UD_P_SNR', 'UD_S_SNR', 'UD_P_CC', 'UD_S_CC', 'UD_N_CC', 'NS_P_SNR', 'NS_S_SNR', 'NS_P_CC', 'NS_S_CC', 'NS_N_CC', 'EW_P_SNR', 'EW_S_SNR', 'EW_P_CC', 'EW_S_CC', 'EW_N_CC'])\n",
    "\n",
    "    c = 0\n",
    "\n",
    "    for fn in files:\n",
    "\n",
    "      if c % 100 == 0:\n",
    "        print(f'{c}/{len(files)}')\n",
    "\n",
    "      data = np.load('../data/Learning'+'/'+fn, allow_pickle=True)\n",
    "      wave, p_onset, s_onset = data['wave'], data['pidx'], data['sidx']\n",
    "      time_str, station_name = os.path.basename(fn).replace('.npz', '').split('_')\n",
    "\n",
    "      # Create ObsPy Stream\n",
    "      original_stream = convert_ndarry_stream(wave-np.mean(wave, axis=1, keepdims=True), time_str, station_name)\n",
    "      denoised_stream = model.annotate(original_stream)\n",
    "\n",
    "      original = convert_stream_to_ndarray(original_stream, channel_order=[\"UD\", \"NS\", \"EW\"])\n",
    "      denoised = convert_stream_to_ndarray(denoised_stream, channel_order=[\"DeepDenoiser_UD\", \"DeepDenoiser_NS\", \"DeepDenoiser_EW\"])\n",
    "\n",
    "      loss, p_snrs, s_snrs, p_ccs, s_ccs, n_ccs = calc_loss(original, denoised, p_onset, s_onset)\n",
    "\n",
    "      writer.writerow([os.path.basename(fn),\n",
    "                       loss,\n",
    "                       p_snrs[0], s_snrs[0], p_ccs[0], s_ccs[0], n_ccs[0],\n",
    "                       p_snrs[1], s_snrs[1], p_ccs[1], s_ccs[1], n_ccs[1],\n",
    "                       p_snrs[2], s_snrs[2], p_ccs[2], s_ccs[2], n_ccs[2]])\n",
    "\n",
    "      # print(os.path.basename(fn), loss, p_snr, s_snr, P_CC, s_cc, n_cc)\n",
    "\n",
    "      total_loss += loss\n",
    "\n",
    "      c += 1\n",
    "\n",
    "print(f'Total Loss = {total_loss/(len(files))}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
